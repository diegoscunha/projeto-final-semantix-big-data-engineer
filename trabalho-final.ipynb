{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final de Spark\n",
    "\n",
    "*Diego Silva Cunha*\n",
    "\n",
    "**Nível Básico:**\n",
    "\n",
    "Dados: https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "Referência das Visualizações:\n",
    "* Site: https://covid.saude.gov.br/\n",
    "* Guia do Site: Painel Geral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Enviar os dados para o hdfs**\n",
    "\n",
    "Salvei os arquivos dentro da pasta /input/dados e faço o envio dos arquivos para o HDFS onde será mapeada a tabela Hive 'painel_covid_temp'.\n",
    "```\n",
    "hdfs dfs -put /input/dados /user/diego/trabalho-final/data/painel_covid_temp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município.**\n",
    "\n",
    "Conectar ao Hive com o comando abaixo:\n",
    "```\n",
    "beeline -u jdbc:hive2://localhost:10000\n",
    "```\n",
    "Criando o banco de dados Hive\n",
    "```\n",
    "create database trabalhofinal;\n",
    "use trabalhofinal;\n",
    "```\n",
    "Criando tabela Hive 'painel_covid_temp'\n",
    "```\n",
    "create external table painel_covid_temp (\n",
    "    regiao string,\n",
    "    estado string,\n",
    "    municipio string,\n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data date,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado int,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana int\n",
    ")\n",
    "row format delimited\n",
    "fields terminated by ';'\n",
    "location'/user/diego/trabalho-final/data/painel_covid_temp'\n",
    "tblproperties(\"skip.header.line.count\"=\"1\");\n",
    "```\n",
    "Realizando a configuração do Hive para suportar particionamento dinâmico.\n",
    "```\n",
    "SET hive.exec.dynamic.partition = true;\n",
    "SET hive.exec.dynamic.partition.mode = nonstrict;\n",
    "```\n",
    "Criando a tabela 'painel_covid' particionada por regiões do Brasil.\n",
    "```\n",
    "create table painel_covid (\n",
    "    estado string,\n",
    "    municipio string,\n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data date,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado int,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana int\n",
    ")\n",
    "partitioned by (regiao string);\n",
    "```\n",
    "Carregando os dados da tabela 'painel_covid_temp' para a tabela particionada por região 'painel_covid'.\n",
    "```\n",
    "insert overwrite table painel_covid partition(regiao) select estado,municipio,coduf,codmun,codRegiaoSaude,nomeRegiaoSaude,data,semanaEpi,populacaoTCU2019,casosAcumulado,casosNovos,obitosAcumulado,obitosNovos,Recuperadosnovos,emAcompanhamentoNovos,interior_metropolitana,regiao from painel_covid_temp;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def calcular_incidencia(casos, populacao, por_habitantes):\n",
    "    return format_number((casos * por_habitantes) / populacao, 2)\n",
    "\n",
    "def calcular_mortalidade(obitos, populacao, por_habitantes):\n",
    "    return format_number((obitos * por_habitantes) / populacao, 2)\n",
    "\n",
    "def calcular_letalidade(obitos, casos):\n",
    "    return format_number((obitos * 100) / casos, 2)\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "spark.catalog.setCurrentDatabase(dbName=\"trabalhofinal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "painel_covid = spark.read.table(\"painel_covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualização 1 - Casos recuperados e em acompahamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|recuperadosnovos|emacompanhamentonovos|\n",
      "+----------------+---------------------+\n",
      "|        17262646|              1317658|\n",
      "+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vizualizacao_1 = painel_covid.select(\"recuperadosnovos\",\"emacompanhamentonovos\")\\\n",
    "            .agg(max(\"recuperadosnovos\").alias(\"recuperadosnovos\"),\\\n",
    "                 max(\"emacompanhamentonovos\").alias(\"emacompanhamentonovos\"))\n",
    "vizualizacao_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualização 2 - Casos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------------+-----------+\n",
      "|casosacumulado|casosnovos|populacaoTCU2019|incidencia*|\n",
      "+--------------+----------+----------------+-----------+\n",
      "|      18855015|    115228|       210147125|  89,722.93|\n",
      "+--------------+----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vizualizacao_2 = painel_covid.select(\"casosacumulado\",\"casosnovos\",\"populacaoTCU2019\")\\\n",
    "            .agg(max(\"casosacumulado\").alias(\"casosacumulado\"),\\\n",
    "                 max(\"casosnovos\").alias(\"casosnovos\"),\\\n",
    "                 max(\"populacaoTCU2019\").alias(\"populacaoTCU2019\"))\\\n",
    "            .withColumn(\"incidencia*\",\n",
    "                        calcular_incidencia(col(\"casosacumulado\").cast(FloatType()),\n",
    "                                            col(\"populacaoTCU2019\").cast(FloatType()),\n",
    "                                            1000000))\n",
    "vizualizacao_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualização 3 - Óbitos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----------+----------------+----------+-----------+\n",
      "|casosacumulado|obitosacumulado|obitosnovos|populacaoTCU2019|letalidade|mortalidade|\n",
      "+--------------+---------------+-----------+----------------+----------+-----------+\n",
      "|      18855015|         526892|       4249|       210147125|      2.79|   2,507.25|\n",
      "+--------------+---------------+-----------+----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vizualizacao_3 = painel_covid.select(\"casosacumulado\",\"obitosacumulado\",\"obitosnovos\",\"populacaoTCU2019\")\\\n",
    "            .agg(max(\"casosacumulado\").alias(\"casosacumulado\"),\n",
    "                 max(\"obitosacumulado\").alias(\"obitosacumulado\"),\n",
    "                 max(\"obitosnovos\").alias(\"obitosnovos\"),\n",
    "                 max(\"populacaoTCU2019\").alias(\"populacaoTCU2019\"))\\\n",
    "            .withColumn(\"letalidade\", \n",
    "                        calcular_letalidade(col(\"obitosacumulado\").cast(FloatType()),\n",
    "                                            col(\"casosacumulado\").cast(FloatType())))\\\n",
    "            .withColumn(\"mortalidade\", \n",
    "                        calcular_mortalidade(col(\"obitosacumulado\").cast(FloatType()),\n",
    "                                             col(\"populacaoTCU2019\").cast(FloatType()),1000000))\n",
    "vizualizacao_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Salvar a primeira visualização como tabela Hive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualizacao_1.write.saveAsTable(\"trabalhofinal.v1_casos_recuperados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Salvar a segunda visualização com formato parquet e compressão snappy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualizacao_2.write.option(\"compression\", \"snappy\")\\\n",
    "                    .parquet(\"/user/diego/trabalho-final/data/v2_casos_confirmados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Salvar a terceira visualização em um tópico no Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualizacao_3.selectExpr(\"CONCAT(1) AS value\", \"CAST(obitosacumulado AS STRING)\",\"CAST(obitosnovos AS STRING)\",\"CAST(letalidade AS STRING)\",\"CAST(mortalidade AS STRING)\")\\\n",
    "            .write\\\n",
    "            .format(\"kafka\")\\\n",
    "            .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "            .option(\"topic\",\"v3-obitos-confirmados\")\\\n",
    "            .option(\"value\", \"1\")\\\n",
    "            .option(\"checkpointLocation\",\"/user/diego/trabalho-final/data/kafka/checkpoint\")\\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Criar a visualização pelo Spark com os dados enviados para o HDFS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------------+----------------+----------------+-----------------+\n",
      "|      regiao|casosacumulado|obitosacumulado|populacaoTCU2019|incidencia100mil|mortalidade100mil|\n",
      "+------------+--------------+---------------+----------------+----------------+-----------------+\n",
      "|    Nordeste|       4426773|         115502|         2872347|      154,116.93|         4,021.17|\n",
      "|         Sul|       3780833|          85038|         1933105|      195,583.43|         4,399.04|\n",
      "|     Sudeste|       7129027|         244856|        12252023|       58,186.53|         1,998.49|\n",
      "|Centro-Oeste|       1916634|          49490|         3015268|       63,564.30|         1,641.31|\n",
      "|      Brasil|      18855015|         526892|       210147125|        8,972.29|           250.73|\n",
      "|       Norte|       1730152|          43929|         2182763|       79,264.31|         2,012.54|\n",
      "+------------+--------------+---------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vizualizacao_regioes = painel_covid.where((col(\"municipio\")!=\"\") | (col(\"regiao\")==\"Brasil\"))\\\n",
    "            .groupBy(\"regiao\",\"municipio\")\\\n",
    "            .agg(max(\"casosacumulado\").alias(\"casosacumulado\"),\n",
    "                 max(\"obitosacumulado\").alias(\"obitosacumulado\"),\n",
    "                 max(\"populacaoTCU2019\").alias(\"populacaoTCU2019\"))\\\n",
    "            .groupBy(\"regiao\")\\\n",
    "            .agg(sum(\"casosacumulado\").alias(\"casosacumulado\"),\n",
    "                 sum(\"obitosacumulado\").alias(\"obitosacumulado\"),\n",
    "                 max(\"populacaoTCU2019\").alias(\"populacaoTCU2019\"))\\\n",
    "            .withColumn(\"incidencia100mil\",\n",
    "                        calcular_incidencia(col(\"casosacumulado\").cast(FloatType()),\n",
    "                                            col(\"populacaoTCU2019\").cast(FloatType()),\n",
    "                                            100000))\\\n",
    "            .withColumn(\"mortalidade100mil\",\n",
    "                        calcular_mortalidade(col(\"obitosacumulado\").cast(FloatType()),\n",
    "                                             col(\"populacaoTCU2019\").cast(FloatType()),\n",
    "                                             100000))\n",
    "vizualizacao_regioes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Salvar a visualização do exercício 6 em um tópico no Elastic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Criar um dashboard no Elastic para visualização dos novos dados enviados**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
